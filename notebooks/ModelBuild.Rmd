---
title: "ngram Dictionary Build"
output: html_notebook
---

This document contains the written and coded logic behind our the ngram lookup dictionary that we use to provide predictions of the next word in a sentence.

# Libraries

```{r libraries, message=FALSE}
library(NextWordR)
library(quanteda)
library(dplyr)
```

# Strategy

The basic goal of this code is to build a table of ngrams which serve as a simple object that we can query to get a complete list of the words to follow based upon one or two previous. This table will have a somewhat rigid structure, with defined fields for first and second words prior to the suggested field. This means that the table will contain some structure that is unecessary overhead, but will allow us to query it in a consistent fashion. Queries will use OR conditions to check the last two words against the first and second position, the last word against the first (with an NA in the second position), or if all else fails an NA in both the first and second position.

The table will be sorted by an algorithm of frequency weights in combination with other factors to ensure that even when we get multiple returns for an ngram, selection of the head will give us the best guess.

An example of what this table would look like is below:

| first | second | suggest | freq | n |
|:-----:|:------:|:-------:|-----:|--:|
| NA    | NA     | the     | 999  | 1 |
| the   | NA     | end     | 888  | 2 |
| the   | end    | is      | 777  | 3 |

We would then perform a query like so:
```{r prototype-query-example, eval=FALSE}
word1 = "the"
word2 = "end"

filter(
  lookup_table,
    (first == word1 | is.na(first))
  & (second == word2 | is.na(second))
)
```

The result of this query would be a match of the first and second words, along with records for NA, which acts as a backoff model in order to let lower level ngrams with higher frequencies take precedence over less frequent bi- and tri-grams.

# Data

We start by reading in our english lanugage datasets, with specific encodings that we've chosen based upon the results of a python utility that examined the files for likely encoding markers. This is still a guess, but we're hoping that it eliminates most issues with character interpretation.

```{r raw-text}
# if you don't have the raw data already, use our function manage_data()
fp = function(x) { file.path('..','data-raw',paste0('en_US.', x,'.txt')) }
txt.twitter = readLines(fp('twitter'), encoding='ISO-8859-2')
txt.blogs = readLines(fp('blogs'), encoding='ISO-8859-2')
txt.news = readLines(fp('news'), encoding='UTF-8')
rm(fp)
```

The twitter file causes some issues due to lines with "embedded nul". We then build this raw text into a `quanteda` corpus object. Due to the large size of these texts, and memory limitations of my workstation, we'll be building our corpus out of a sample of each body of text. However, this should still be a statistically relevant data set since we'll be performing a random 1% (this number is arbitrary) sample of the same proportion from each source.

```{r sample-text}
set.seed(57130)
txt = 
    list(txt.twitter, txt.blogs, txt.news) %>%
        sapply(., function(data) {
            ix = caret::createDataPartition(1:length(data), p=0.001, list=FALSE)
            data[ix]
        }) %>% unlist
```

Once our sample text is built, we dump the complete raw objects and recover some memory.

```{r dump}
rm(txt.blogs, txt.news, txt.twitter)
gc()
```

Then we build this into a corpus.

```{r corpus}
corp = corpus(txt)
```

At this point, we perform the most memory intensive step, building our document feature matrix, which includes ngrams of length 1 to 3. It is important to take note of the options used to preprocess the corpus, as those same preprocessing steps will need to be applied to our input text at processing.

```{r document-feature-matrix}
docfeatmat = dfm(
  corp,
  ngrams=1:3, tolower=TRUE,
  removeNumbers = TRUE, removePunct = TRUE,
  removeSymbols = TRUE
)
```

```{r}
docfeatmat
```


And then attempt to convert the ngram data into a table that will fit with our prototype.

```{r}
# TODO: this is horrible. Fix this.
ngram_lookup = docfreq(docfeatmat) %>%
  data.frame(gram=names(.), freq=., stringsAsFactors=FALSE)

ngram_lookup = ngram_lookup %>% 
  tidyr::separate(gram, c("first", "second", "third"), sep = "_", remove=FALSE)

ngram_lookup = ngram_lookup %>% 
  mutate(
    nlength = as.integer(NA),
    nlength = ifelse(!is.na(third)  & is.na(nlength), 3L, nlength),
    nlength = ifelse(!is.na(second) & is.na(nlength), 2L, nlength),
    nlength = ifelse(!is.na(first)  & is.na(nlength), 1L, nlength)
  ) %>% 
  transmute(
    key1 = as.factor(ifelse(nlength > 1, first, NA)),
    key2 = as.factor(ifelse(nlength > 2, second, NA)),
    suggest = NA,
    suggest = ifelse(nlength == 1, first, suggest),
    suggest = ifelse(nlength == 2, second, suggest),
    suggest = ifelse(nlength == 3, third, suggest),
    freq,
    nlength
  )

ngram_lookup = ngram_lookup %>%
  group_by(nlength) %>%
  mutate(prop = freq / sum(nlength)) %>%
  arrange(
    desc(nlength),
    desc(prop)
  )
```

And now we save our ngram lookup dictionary so that we can access this object in our other scripts (including the nextword application).

```{r export}
saveRDS(ngram_lookup, 'ngram_lookup.Rds')
```

