---
title: "Dictionary Build"
output: html_notebook
---

# Libraries

We'll make extensive use of the pipe command `%>%` from magrittr, and the bulk of our text mining tools will come from the `quanteda` package. Manipulation of more traditional R objects will be handled by `dplyr`. Additionally, we borrow limited tools from `caret` and `scales`, but those namespaces will be specified explicitly when they are needed.

```{r libraries, message=FALSE}
library(magrittr)
library(quanteda)
library(dplyr)
```

# Strategy

The basic goal of this code is to build a table of ngrams which serve as a simple queryable object that gives us a "prediction" of the most likely next word based upon a query against the first and second words in the gram. This table will have a somewhat rigid structure, with defined fields for first, second, and third words. This means that the table will contain some structure that is unecessary overhead, but will allow us to query it in a consistent fashion. Sorting of the table by frequency weights will ensure that even when we get multiple returns for an ngram, selection of the head will give us the best guess. The head argument can be paramaterized as well to return additional suggestions in probability order.

An example of what this table would look like is below:

| first | second | suggest | freq | n |
|:-----:|:------:|:-------:|-----:|--:|
| NA    | NA     | the     | 999  | 1 |
| the   | NA     | end     | 888  | 2 |
| the   | end    | is      | 777  | 3 |

We would then perform a query like so:
```{r prototype-query-example, eval=FALSE}
word1 = "the"
word2 = "end"

filter(
  lookup_table,
    (first == word1 | is.na(first))
  & (second == word2 | is.na(second))
)
```

The result of this query would be a match of the first and second words, along with records for NA, which acts as a backoff model in order to let lower level ngrams with higher frequencies take precedence over less frequent bi- and tri-grams.

# Data

We're going to read in our english lanugage datasets, with specific codings that we've chosen based upon the results of a python utility that examined the files for likely encoding markers.

```{r raw-text}
fp = function(x) { file.path('data',paste0('en_US.', x,'.txt')) }
txt.twitter = readLines(fp('twitter'), encoding='ISO-8859-2')
txt.blogs = readLines(fp('blogs'), encoding='ISO-8859-2')
txt.news = readLines(fp('news'), encoding='UTF-8')
rm(fp)
```

The twitter file causes some issues due to lines with "embedded nul". We then build this raw text into a `quanteda` corpus object. Due to the large size of these texts, and memory limitations of my workstation, we'll be building our corpus out of a sample of each body of text. However, this should still be a statistically relevant data set since we'll be performing a random 1% (this number is arbitrary) sample of the same proportion from each source.

```{r sample-text}
set.seed(57130)
txt = 
    list(txt.twitter, txt.blogs, txt.news) %>%
        sapply(., function(data) {
            ix = caret::createDataPartition(1:length(data), p=0.001, list=FALSE)
            data[ix]
        }) %>% unlist
```

Once our sample text is built, we dump the complete raw objects and recover some memory.

```{r dump}
rm(txt.blogs, txt.news, txt.twitter)
gc()
```

```{r corpus}
corp = corpus(txt)
```

At this point, we perform the very memory intensive step, building our document feature matrix.

```{r document-feature-matrix}
docfeatmat = dfm(
  corp,
  ngrams=1:3, tolower=TRUE,
  removeNumbers = TRUE, removePunct = TRUE,
  removeSymbols = TRUE
)
```

And then attempt to convert the ngram data into a table that will fit with our prototype.

```{r}
# TODO: this is horrible. Fix this.
ngram_lookup = docfreq(docfeatmat) %>%
  data.frame(gram=names(.), freq=., stringsAsFactors=FALSE)

ngram_lookup = ngram_lookup %>% 
  tidyr::separate(gram, c("first", "second", "third"), sep = "_", remove=FALSE)

ngram_lookup = ngram_lookup %>% 
  mutate(
    nlength = as.integer(NA),
    nlength = ifelse(!is.na(third)  & is.na(nlength), 3L, nlength),
    nlength = ifelse(!is.na(second) & is.na(nlength), 2L, nlength),
    nlength = ifelse(!is.na(first)  & is.na(nlength), 1L, nlength)
  ) %>% 
  transmute(
    key1 = as.factor(ifelse(nlength > 1, first, NA)),
    key2 = as.factor(ifelse(nlength > 2, second, NA)),
    suggest = NA,
    suggest = ifelse(nlength == 1, first, suggest),
    suggest = ifelse(nlength == 2, second, suggest),
    suggest = ifelse(nlength == 3, third, suggest),
    freq,
    nlength
  )

ngram_lookup = ngram_lookup %>%
  group_by(nlength) %>%
  mutate(prop = freq / sum(nlength)) %>%
  arrange(
    desc(nlength),
    desc(prop)
  )
```

And now we save our ngram lookup dictionary so that we can access this object in our other scripts (including the nextword application).

```{r export}
saveRDS(ngram_lookup, 'ngram_lookup.Rds')
```

