---
title: "ngram Dictionary Build"
output: html_document
---

This document contains the written and coded logic behind our the ngram lookup dictionary that we use to provide predictions of the next word in a sentence.

# Libraries

```{r libraries}
library(quanteda)
suppressMessages(library(dplyr))
```

# Strategy

The basic goal of this code is to build a table of ngrams which serve as a simple object that we can query to get a complete list of the words to follow based upon one or two previous. This table will have a somewhat rigid structure, with defined fields for first and second words prior to the suggested field. This means that the table will contain some structure that is unecessary overhead, but will allow us to query it in a consistent fashion. Queries will use OR conditions to check the last two words against the first and second position, the last word against the first (with an NA in the second position), or if all else fails an NA in both the first and second position.

The table will be sorted by an algorithm of frequency weights in combination with other factors to ensure that even when we get multiple returns for an ngram, selection of the head will give us the best guess.

An example of what this table would look like is below:

| first | second | suggest | freq | n |
|:-----:|:------:|:-------:|-----:|--:|
| NA    | NA     | the     | 999  | 1 |
| the   | NA     | end     | 888  | 2 |
| the   | end    | is      | 777  | 3 |

We would then perform a query like so:
```{r prototype-query-example, eval=FALSE}
word1 = "the"
word2 = "end"

filter(
  lookup_table,
    (first == word1 | is.na(first))
  & (second == word2 | is.na(second))
)
```

The result of this query would be a match of the first and second words, along with records for NA, which acts as a backoff model in order to let lower level ngrams with higher frequencies take precedence over less frequent bi- and tri-grams.

# Data

We start by reading in our english lanugage datasets, with specific encodings that we've chosen based upon the results of a python utility that examined the files for likely encoding markers. This is still a guess, but we're hoping that it eliminates most issues with character interpretation.

```{r raw-text}
# if you don't have the raw data already, use our function manage_data()
fp = function(x) { file.path(paste0('en_US.', x,'.txt')) }
txt.twitter = readLines(fp('twitter'), encoding='ISO-8859-2')
txt.blogs = readLines(fp('blogs'), encoding='ISO-8859-2')
txt.news = readLines(fp('news'), encoding='UTF-8')
rm(fp)
```

The twitter file causes some issues due to lines with "embedded nul". We then build this raw text into a `quanteda` corpus object. Due to the large size of these texts, and memory limitations of my workstation, we'll be building our corpus out of a sample of each body of text. However, this should still be a statistically relevant data set since we'll be performing a random 8% (this number is arbitrary) sample. We'll sample the same  proportion from each source to ensure fair representation of each type of text body.

```{r sample-text}
set.seed(57130)
txt = 
  list(txt.twitter, txt.blogs, txt.news) %>%
    sapply(., function(data) {
      ix = caret::createDataPartition(1:length(data), p=0.04, list=FALSE)
      data[ix]
    }) %>% unlist
```

We'll ignore the EOF errors. Once our sample text is built, we dump the complete raw objects and recover some memory.

```{r dump}
rm(txt.blogs, txt.news, txt.twitter)
gc()
```

Then we build this into a corpus.

```{r corpus}
corp = corpus(txt)
tokenize()
```

At this point, we perform the most memory intensive step, building our document feature matrix, which includes ngrams of length 1 to 3. It is important to take note of the options used to preprocess the corpus, as those same preprocessing steps will need to be applied to our input text at processing.

```{r document-feature-matrix}
dfm1 = dfm(
  corp, ngrams=1, tolower=TRUE, concatenator = " ",
  remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE
)
saveRDS(dfm1, 'dfm1.rds')
rm(dfm1)
gc()


dfm2 = dfm(
  corp, ngrams=2, tolower=TRUE, concatenator = " ",
  remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE
)
saveRDS(dfm2, 'dfm2.rds')
rm(dfm2)
gc()


dfm3 = dfm(
  corp, ngrams=3, tolower=TRUE, concatenator = " ",
  remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE
)
saveRDS(dfm3, 'dfm3.rds')
rm(dfm3)
gc()

rm(corp, txt)
gc()
```

Before we can implement our back-off model we first need to estimate the actual prevalence of ngrams "in the wild" based upon the samples in our corpus. For this we will use Good-Turing estimation. Additionally, after we determine our Good-Turing proporation we can discard the data that is very unlikely to be useful in our model. Specifically, we will remove all ngrams that have a frequency of 1 in our sample. We're doing this at this point primarily for computational performance reasons.

```{r Good-Turing Estimation}
dfm1 = readRDS(file.path('dfm1.rds'))
df1 = docfreq(dfm1) %>%
  data.frame(
    gram = names(.),
    freq = .,
    gtprop = edgeR::goodTuringProportions(.),
    stringsAsFactors=FALSE
  ) %>%
  filter(freq > 1) %>%
  tibble::as_tibble(.) %>%
  arrange(desc(gtprop))

dfm2 = readRDS(file.path('dfm2.rds'))
df2 = docfreq(dfm2) %>%
  data.frame(
    gram = names(.),
    freq = .,
    gtprop = edgeR::goodTuringProportions(.),
    stringsAsFactors=FALSE
  ) %>%
  filter(freq > 1) %>%
  tibble::as_tibble(.) %>%
  arrange(desc(gtprop))

dfm3 = readRDS(file.path('dfm3.rds'))
df3 = docfreq(dfm3) %>%
  data.frame(
    gram = names(.),
    freq = .,
    gtprop = edgeR::goodTuringProportions(.),
    stringsAsFactors=FALSE
  ) %>%
  filter(freq > 1) %>%
  tibble::as_tibble(.) %>%
  arrange(desc(gtprop))

rm(dfm1, dfm2, dfm3)
gc()
```

We need to cast our ngram data frames into a more consistent shape so that we can eventually bind them together. For the monograms this just means renaming, but for our bi- and trigrams we need to separate the gram column by the space character.

```{r monogram-prep}
df1 = df1 %>%
  transmute(
    key1 = as.character(NA),
    key2 = as.character(NA),
    suggest = gram,
    freq,
    gtprop,
    gramlength = 1L
  )
```

The bigram requires a step that the monogram didn't, which is the separation of the key word and the suggestion (the monogram didn't have any keywords). We do this with the `tidyr` package.

```{r bigram-prep}
df2 = df2 %>%
  tidyr::separate(gram, c("key1", "suggest"), sep = " ", remove=TRUE) %>% 
  transmute(
    key1,
    key2 = as.character(NA),
    suggest,
    freq,
    gtprop,
    gramlength = 2L
  )
```

And a similar process with the trigram table.

```{r trigram-prep}
df3 = df3 %>%
  tidyr::separate(gram, c("key1", "key2", "suggest"), sep = " ", remove=TRUE) %>%
  transmute(
    key1,
    key2,
    suggest,
    freq,
    gtprop,
    gramlength = 3L
  )
```

Now we're ready to bind our ngram tables together, and we can start thinking a bit more about the end product. How many word suggestions could possibly be useful? For live text prediction app, it seems unreasonable to display for than 5 words at a time. So in each possible "scenario"" of making suggestion, we'll limit our table to 5 records.

If we consider the grouping of key1 and key2 as a composite key which comprises a "scenario", we can partition our ngram tables by this key and perform a ranking of every suggestion by it's gtprop. In fact we're just using the `row_number` function, but because we've ordered our ngram tables by descending `grprop`, row number is effectively the same thing.

Accordingly, we can then filter the entire table by keygroup rank to limit to just the top 5 suggestions that we could make for every scenario that we are aware of.

```{r mastergram}
mastergram = bind_rows(df1, df2, df3) %>%
  group_by(key1, key2) %>%  # define our key grouping
  mutate(keygroup_rank = row_number()) %>%  # perform ranking
  filter(keygroup_rank <= 5) %>%  # limit to top 5 suggestions for each key group
  group_by() %>%  # release grouping
  transmute(
    key1 = as.factor(key1),
    key2 = as.factor(key2),
    suggest = as.factor(suggest),
    gtprop,
    gramlength
  )
```

This leaves us with a fairly compact ngram table of `r format(object.size(mastergram),'MB')`.

```{r}
saveRDS(mastergram, 'mastergram.rda')
```

And now we have to decide how our backoff model should work.
