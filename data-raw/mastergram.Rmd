---
title: "ngram Dictionary Build"
output: html_document
---

This document contains the written and coded logic behind our the ngram lookup dictionary that we use to provide predictions of the next word in a sentence.

# Libraries

```{r libraries}
library(quanteda)
suppressMessages(library(dplyr))
```

# Strategy

The basic goal of this code is to build a table of ngrams which serve as a simple object that we can query to get a complete list of the words to follow based upon one or two previous. This table will have a somewhat rigid structure, with defined fields for first and second words prior to the suggested field. This means that the table will contain some structure that is unecessary overhead, but will allow us to query it in a consistent fashion. Queries will use OR conditions to check the last two words against the first and second position, the last word against the first (with an NA in the second position), or if all else fails an NA in both the first and second position.

The table will be sorted by an algorithm of frequency weights in combination with other factors to ensure that even when we get multiple returns for an ngram, selection of the head will give us the best guess.

An example of what this table would look like is below:

| first | second | suggest | freq | n |
|:-----:|:------:|:-------:|-----:|--:|
| NA    | NA     | the     | 999  | 1 |
| the   | NA     | end     | 888  | 2 |
| the   | end    | is      | 777  | 3 |

We would then perform a query like so:
```{r prototype-query-example, eval=FALSE}
word1 = "the"
word2 = "end"

filter(
  lookup_table,
    (first == word1 | is.na(first))
  & (second == word2 | is.na(second))
)
```

The result of this query would be a match of the first and second words, along with records for NA, which acts as a backoff model in order to let lower level ngrams with higher frequencies take precedence over less frequent bi- and tri-grams.

# Data

We start by reading in our english lanugage datasets, with specific encodings that we've chosen based upon the results of a python utility that examined the files for likely encoding markers. This is still a guess, but we're hoping that it eliminates most issues with character interpretation.

```{r raw-text}
# if you don't have the raw data already, use our function manage_data()
fp = function(x) { file.path(paste0('en_US.', x,'.txt')) }
txt.twitter = readLines(fp('twitter'), encoding='ISO-8859-2')
txt.blogs = readLines(fp('blogs'), encoding='ISO-8859-2')
txt.news = readLines(fp('news'), encoding='UTF-8')
rm(fp)
```

The twitter file causes some issues due to lines with "embedded nul". Considering how much data we're getting even with those errors, we're just going to let them go. Next we build this raw text into a `quanteda` corpus object. Due to the large size of these texts, and memory limitations of my workstation, we'll be building our corpus out of a sample of each body of text. However, this should still be a statistically relevant data set since we'll be performing a random 4% (this number is arbitrary) sample. We'll sample the same proportion from each source to ensure fair representation of each type of text body.

```{r sample-text}
set.seed(57130)
txt = 
  list(txt.twitter, txt.blogs, txt.news) %>%
    sapply(., function(data) {
      ix = caret::createDataPartition(1:length(data), p=0.04, list=FALSE)
      data[ix]
    }) %>% unlist
```

We'll ignore the EOF errors. Once our sample text is built, we dump the raw objects and recover some memory. Then we build this into a corpus.

```{r dump}
rm(txt.blogs, txt.news, txt.twitter)
gc()
corp = corpus(txt)
```

At this point, we perform the most memory intensive step, building our document feature matrix, which includes ngrams of length 1 to 3. It is important to take note of the options used to preprocess the corpus, as those same preprocessing steps will need to be applied to our input text at processing. We'll be saving our document feature matrixes along the way in case R crashes.

```{r document-feature-matrix}
dfm1 = dfm(
  corp, ngrams=1, tolower=TRUE, concatenator = " ",
  remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE
)
saveRDS(dfm1, 'dfm1.rds')
rm(dfm1)
gc()


dfm2 = dfm(
  corp, ngrams=2, tolower=TRUE, concatenator = " ",
  remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE
)
saveRDS(dfm2, 'dfm2.rds')
rm(dfm2)
gc()


dfm3 = dfm(
  corp, ngrams=3, tolower=TRUE, concatenator = " ",
  remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE
)
saveRDS(dfm3, 'dfm3.rds')
rm(dfm3)
gc()

rm(corp, txt)
gc()
```

And now we have a objects for mono-, bi-, and trigrams. Eventually we'll combine these as a sort of lookup dictionary, but first need to estimate the actual prevalence of ngrams "in the wild" based upon the samples in our corpus. This is important when we implement our back-off model.

For this task we will use Good-Turing estimation. Additionally, after we determine our Good-Turing proporation we can discard the data that is very unlikely to be useful in our model. Specifically, we will remove all ngrams that have a frequency of 1 in our sample. We're doing this as soon as possible primarily for computational performance reasons.

## Data Frame

```{r Good-Turing Estimation}
dfm1 = readRDS(file.path('dfm1.rds'))
df1 = docfreq(dfm1) %>%
  data.frame(
    gram = names(.),
    freq = .,
    gtprop = edgeR::goodTuringProportions(.),
    stringsAsFactors=FALSE
  ) %>%
  tibble::as_tibble(.) %>%
  arrange(desc(freq))

dfm2 = readRDS(file.path('dfm2.rds'))
df2 = docfreq(dfm2) %>%
  data.frame(
    gram = names(.),
    freq = .,
    stringsAsFactors=FALSE
  ) %>%
  tibble::as_tibble(.) %>%
  arrange(desc(freq))

dfm3 = readRDS(file.path('dfm3.rds'))
df3 = docfreq(dfm3) %>%
  data.frame(
    gram = names(.),
    freq = .,
    stringsAsFactors=FALSE
  ) %>%
  tibble::as_tibble(.) %>%
  arrange(desc(freq))

rm(dfm1, dfm2, dfm3)
gc()
```

We need to cast our ngram data frames into a more consistent shape so that we can eventually bind them together. For the monograms this just means renaming, but for our bi- and trigrams we need to separate the gram column by the space character.

```{r monogram-prep}
df1 = rename(df1, suggest = gram)
```


The bigram requires a step that the monogram didn't, which is the separation of the key word and the suggestion (the monogram didn't have any keywords). We do this with the `tidyr` package.

```{r bigram-prep}
df2 = df2 %>%
  tidyr::separate(gram, c("key1", "suggest"), sep = " ", remove=TRUE) %>% 
  transmute(
    key1,
    suggest,
    freq
  )
```

```{r backoff}
library(edgeR)
library(data.table)

keep = 15   # number of suggestions to keep. Applied at each level
keygroups = select(df2, key1) %>% unique
keygroups = keygroups[1:10000,]

dt1 = as.data.table(df1)
dt1 = dt1[order(freq)]
dt2 = as.data.table(df2) %>% setkey(key1, freq)

bigrams = 
  apply(keygroups, 1, function(x) {
    # subset keygroup records
    sugg = dt2[(x[['key1']])]
    # calculate proportions for seen records
    sugg[,gtprop := goodTuringProportions(freq)]
    # calculate prevalence of unseen grams
    P0 = goodTuring(sugg$freq)$P0
    # calculate proportion of dt1 that's already covered in dt2
    lower_CoverP = dt1[suggest %in% sugg$suggest, gtprop] %>% sum
    # remove suggestions already in higher ngram, keep select amount
    sugg_lower = dt1[suggest %in% sugg$suggest, tail(.SD, keep)] 
    # rescale prop after removing covered ngrams proportion from 1
    sugg_lower[, gtprop := gtprop / (1 - lower_CoverP)] 
    # rescale prop as share of P0
    sugg_lower[, gtprop := gtprop * P0] 
    # add key value and ngram length to dt1
    sugg_lower[, key1 := x[['key1']] ]
    sugg_lower[, ngramL := 1L ]
    # add ngram length to dt2
    sugg[, ngramL := 2L ]
    # bind lower level ngram with higher
    combined = rbindlist(list(sugg, sugg_lower), use.names=TRUE, fill = FALSE)
    # keep only the top select amount
    combined[order(gtprop), tail(.SD, keep)]
  }) %>%
  rbindlist(., use.names=FALSE, fill = FALSE) %>%
  setkey(key1, gtprop)
```

And a similar process with the trigram table.

```{r trigram-prep}
df3 = df3 %>%
  tidyr::separate(gram, c("key1", "key2", "suggest"), sep = " ", remove=TRUE) %>%
  transmute(
    key1,
    key2,
    suggest,
    freq,
    gtprop,
    gramlength = 3L
  )
```

Now we're ready to bind our ngram tables together, and we can start thinking a bit more about the end product. How many word suggestions could possibly be useful? For live text prediction app, it seems unreasonable to display for than 5 words at a time. So in each possible "scenario"" of making suggestions, we'll limit our table to 5 records.

If we consider the grouping of key1 and key2 as a composite key which comprises a "scenario", we can partition our ngram tables by this key and perform a ranking of every suggestion by it's `gtprop` value. In fact we're just using the `row_number` function, but because we've ordered our ngram tables by descending `gtprop`, `row_number` is effectively the same. We can then filter the entire table by `keygroup_rank` to limit to just the top 5 suggestions that we could make for every scenario that we are aware of.

```{r mastergram}
mastergram = bind_rows(df1, df2, df3) %>%  # combine our ngram tables into one
  group_by(key1, key2) %>%  # define our key grouping in the table
  mutate(keygroup_rank = row_number()) %>%  # perform ranking of each scenario
  filter(keygroup_rank <= 5) %>%  # limit to top 5 suggestions for each scenario
  group_by() %>%  # release grouping on table
  transmute(  # clean up table, and cast characters as factors (for performance)
    key1 = as.factor(key1),
    key2 = as.factor(key2),
    suggest = as.factor(suggest),
    gtprop,
    gramlength
  )
```

This leaves us with a fairly compact ngram table of `r format(object.size(mastergram),'MB')`. We'll save that for later.

```{r}
saveRDS(mastergram, 'mastergram.rda')
```

And now we have to decide how our backoff model should work.

```{r}
mastergram = readRDS('mastergram.rda')
```

